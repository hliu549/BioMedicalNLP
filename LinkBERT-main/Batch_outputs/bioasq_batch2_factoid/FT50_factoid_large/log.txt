Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
04/13/2023 16:10:57 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
04/13/2023 16:10:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=None,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=../Batch_outputs/bioasq_batch2_factoid/FT50_factoid_large/runs/Apr13_16-10-56_linkbert-1,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=50.0,
output_dir=../Batch_outputs/bioasq_batch2_factoid/FT50_factoid_large,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=FT50_factoid_large,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=../Batch_outputs/bioasq_batch2_factoid/FT50_factoid_large,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.NO,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
04/13/2023 16:10:58 - WARNING - datasets.builder - Using custom data configuration default-dcee098aab16cef3
04/13/2023 16:10:58 - INFO - datasets.builder - Overwrite dataset info from restored data version.
04/13/2023 16:10:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dcee098aab16cef3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264
04/13/2023 16:10:58 - WARNING - datasets.builder - Reusing dataset json (/root/.cache/huggingface/datasets/json/default-dcee098aab16cef3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264)
04/13/2023 16:10:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dcee098aab16cef3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 153.53it/s]
[INFO|configuration_utils.py:543] 2023-04-13 16:10:58,045 >> loading configuration file ../experiments/bioasq_factoid_50/BioLinkBERT-large/config.json
[INFO|configuration_utils.py:581] 2023-04-13 16:10:58,046 >> Model config BertConfig {
  "_name_or_path": "../models/BioLinkBERT-large",
  "architectures": [
    "BertForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.9.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28895
}

[INFO|tokenization_utils_base.py:1664] 2023-04-13 16:10:58,047 >> Didn't find file ../experiments/bioasq_factoid_50/BioLinkBERT-large/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1728] 2023-04-13 16:10:58,047 >> loading file ../experiments/bioasq_factoid_50/BioLinkBERT-large/vocab.txt
[INFO|tokenization_utils_base.py:1728] 2023-04-13 16:10:58,048 >> loading file ../experiments/bioasq_factoid_50/BioLinkBERT-large/tokenizer.json
[INFO|tokenization_utils_base.py:1728] 2023-04-13 16:10:58,048 >> loading file None
[INFO|tokenization_utils_base.py:1728] 2023-04-13 16:10:58,048 >> loading file ../experiments/bioasq_factoid_50/BioLinkBERT-large/special_tokens_map.json
[INFO|tokenization_utils_base.py:1728] 2023-04-13 16:10:58,049 >> loading file ../experiments/bioasq_factoid_50/BioLinkBERT-large/tokenizer_config.json
[INFO|modeling_utils.py:1269] 2023-04-13 16:10:58,069 >> loading weights file ../experiments/bioasq_factoid_50/BioLinkBERT-large/pytorch_model.bin
[INFO|modeling_utils.py:1510] 2023-04-13 16:11:02,413 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.

[INFO|modeling_utils.py:1518] 2023-04-13 16:11:02,414 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ../experiments/bioasq_factoid_50/BioLinkBERT-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.
04/13/2023 16:11:02 - INFO - datasets.arrow_dataset - Spawning 10 processes
04/13/2023 16:11:03 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on prediction dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]04/13/2023 16:11:03 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Running tokenizer on prediction dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s][ARunning tokenizer on prediction dataset #0: 100%|██████████| 1/1 [00:00<00:00,  5.04ba/s]04/13/2023 16:11:03 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on prediction dataset #0: 100%|██████████| 1/1 [00:00<00:00,  4.23ba/s]


Running tokenizer on prediction dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s][A[A
Running tokenizer on prediction dataset #1: 100%|██████████| 1/1 [00:00<00:00,  5.05ba/s][ARunning tokenizer on prediction dataset #1: 100%|██████████| 1/1 [00:00<00:00,  5.04ba/s]


Running tokenizer on prediction dataset #2: 100%|██████████| 1/1 [00:00<00:00,  7.58ba/s][A[ARunning tokenizer on prediction dataset #2: 100%|██████████| 1/1 [00:00<00:00,  7.12ba/s]
04/13/2023 16:11:03 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.



Running tokenizer on prediction dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A04/13/2023 16:11:03 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.




Running tokenizer on prediction dataset #4:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A


Running tokenizer on prediction dataset #3: 100%|██████████| 1/1 [00:00<00:00,  5.02ba/s][A[A[ARunning tokenizer on prediction dataset #3: 100%|██████████| 1/1 [00:00<00:00,  4.83ba/s]
04/13/2023 16:11:04 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.





Running tokenizer on prediction dataset #5:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A



Running tokenizer on prediction dataset #4: 100%|██████████| 1/1 [00:00<00:00,  3.28ba/s][A[A[A[ARunning tokenizer on prediction dataset #4: 100%|██████████| 1/1 [00:00<00:00,  3.15ba/s]
04/13/2023 16:11:04 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.






Running tokenizer on prediction dataset #6:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A




Running tokenizer on prediction dataset #5: 100%|██████████| 1/1 [00:00<00:00,  3.50ba/s][A[A[A[A[ARunning tokenizer on prediction dataset #5: 100%|██████████| 1/1 [00:00<00:00,  3.42ba/s]
04/13/2023 16:11:04 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.







Running tokenizer on prediction dataset #7:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A





Running tokenizer on prediction dataset #6: 100%|██████████| 1/1 [00:00<00:00,  4.54ba/s][A[A[A[A[A[ARunning tokenizer on prediction dataset #6: 100%|██████████| 1/1 [00:00<00:00,  4.45ba/s]
04/13/2023 16:11:04 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.








Running tokenizer on prediction dataset #8:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A






Running tokenizer on prediction dataset #7: 100%|██████████| 1/1 [00:00<00:00,  4.96ba/s][A[A[A[A[A[A[ARunning tokenizer on prediction dataset #7: 100%|██████████| 1/1 [00:00<00:00,  4.81ba/s]
04/13/2023 16:11:04 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.prepare_validation_features at 0x7f4720bb98b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.









Running tokenizer on prediction dataset #9:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A







Running tokenizer on prediction dataset #8: 100%|██████████| 1/1 [00:00<00:00,  6.46ba/s][A[A[A[A[A[A[A[ARunning tokenizer on prediction dataset #8: 100%|██████████| 1/1 [00:00<00:00,  5.79ba/s]









Running tokenizer on prediction dataset #9: 100%|██████████| 1/1 [00:00<00:00,  9.11ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on prediction dataset #9: 100%|██████████| 1/1 [00:00<00:00,  8.70ba/s]
04/13/2023 16:11:05 - INFO - datasets.arrow_dataset - Concatenating 10 shards from multiprocessing
04/13/2023 16:11:05 - WARNING - datasets.fingerprint - Transform <function concatenate_datasets at 0x7f48b25500d0> couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
04/13/2023 16:11:06 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad
04/13/2023 16:11:06 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e
04/13/2023 16:11:06 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py to /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/squad.py
04/13/2023 16:11:06 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/dataset_infos.json
04/13/2023 16:11:06 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/squad.json
04/13/2023 16:11:06 - INFO - datasets.load - Found local import file from /root/.cache/huggingface/datasets/downloads/188f3c7a325773b47d41f3e0f7ab9fd3cb20e597010b3a9d780c878dacc10ce3.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/evaluate.py
[INFO|trainer.py:421] 2023-04-13 16:11:21,508 >> Using amp fp16 backend
04/13/2023 16:11:21 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:521] 2023-04-13 16:11:21,509 >> The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.
[INFO|trainer.py:2165] 2023-04-13 16:11:21,569 >> ***** Running Prediction *****
[INFO|trainer.py:2167] 2023-04-13 16:11:21,575 >>   Num examples = 61
[INFO|trainer.py:2170] 2023-04-13 16:11:21,575 >>   Batch size = 8
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:00<00:00,  9.43it/s] 38%|███▊      | 3/8 [00:00<00:00,  6.37it/s] 50%|█████     | 4/8 [00:00<00:00,  5.76it/s] 62%|██████▎   | 5/8 [00:00<00:00,  5.45it/s] 75%|███████▌  | 6/8 [00:01<00:00,  5.21it/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.72it/s]100%|██████████| 8/8 [00:01<00:00,  5.36it/s]04/13/2023 16:11:23 - INFO - utils_qa - Post-processing 61 example predictions split into 61 features.

  0%|          | 0/61 [00:00<?, ?it/s][A
 18%|█▊        | 11/61 [00:00<00:00, 109.69it/s][A
 36%|███▌      | 22/61 [00:00<00:00, 90.44it/s] [A
 52%|█████▏    | 32/61 [00:00<00:00, 84.98it/s][A
 74%|███████▍  | 45/61 [00:00<00:00, 100.40it/s][A100%|██████████| 61/61 [00:00<00:00, 109.40it/s]
04/13/2023 16:11:24 - INFO - utils_qa - Saving predictions to ../Batch_outputs/bioasq_batch2_factoid/FT50_factoid_large/predict_predictions.json.
04/13/2023 16:11:24 - INFO - utils_qa - Saving nbest_preds to ../Batch_outputs/bioasq_batch2_factoid/FT50_factoid_large/predict_nbest_predictions.json.
Traceback (most recent call last):
  File "qa/run_qa.py", line 663, in <module>
    main()
  File "qa/run_qa.py", line 629, in main
    results = trainer.predict(predict_dataset, predict_examples)
  File "/home/haiyingl/BioMedicalNLP/LinkBERT-main/src/qa/trainer_qa.py", line 98, in predict
    metrics = self.compute_metrics(predictions)
  File "qa/run_qa.py", line 576, in compute_metrics
    return metric.compute(predictions=p.predictions, references=p.label_ids)
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/metric.py", line 390, in compute
    self.add_batch(predictions=predictions, references=references)
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/metric.py", line 431, in add_batch
    batch = self.info.features.encode_batch(batch)
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/features.py", line 1034, in encode_batch
    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/features.py", line 1034, in <listcomp>
    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/features.py", line 864, in encode_nested_example
    return {
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/features.py", line 865, in <dictcomp>
    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/features.py", line 882, in encode_nested_example
    for k, (sub_schema, sub_objs) in utils.zip_dict(schema.feature, obj):
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 99, in zip_dict
    yield key, tuple(d[key] for d in dicts)
  File "/opt/conda/envs/linkbert/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 99, in <genexpr>
    yield key, tuple(d[key] for d in dicts)
KeyError: 'l'
100%|██████████| 8/8 [00:02<00:00,  3.29it/s]
